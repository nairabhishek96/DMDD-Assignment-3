{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Scraping Twitter: University Marketplace</h3>\n",
    "\n",
    "Introduction\n",
    "\n",
    "Here we are using twitter API to extract tweets related to buy and sell of used items in twitter. After extracting all the tweets using the API related to our search value, we will save all the records in our MySQL tables (tweet, tweet_mentions, tweet_tags). \n",
    "\n",
    "<h3>Importing Essential Libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import configparser\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "import matplotlib\n",
    "pymysql.install_as_MySQLdb()\n",
    "import mysql.connector\n",
    "from mysql.connector import Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Authentication keys</h3>\n",
    "\n",
    "Here we are defining keys to authenticate with twitter API and start calling API functions to extract tweets for our analysis.\n",
    "\n",
    "You need to register for a Twitter dev account https://developer.twitter.com\n",
    "\n",
    "Look at the Twitter data model https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/tweet\n",
    "\n",
    "Apply for a Twitter Developer Account\n",
    "\n",
    "Go to the Twitter developer site to apply for a developer account.\n",
    "\n",
    "Step 2: Create an Application\n",
    "\n",
    "Twitter grants authentication credentials to apps, not accounts. An app can be any tool or bot that uses the Twitter API. So you need to register your an app to be able to make API calls.\n",
    "\n",
    "To register your app, go to your Twitter apps page and select the Create an app option.\n",
    "\n",
    "You need to provide the following information about your app and its purpose:\n",
    "\n",
    "App name: a name to identify your application (such as examplebot) Application description: the purpose of your application (such as An example bot for a Real Python article) Your or your application’s website URL: required, but can be your personal site’s URL since bots don’t need a URL to work Use of the app: how users will use your app (such as This app is a bot that will automatically respond to users) Step 3: Create the Authentication Credentials\n",
    "\n",
    "To create the authentication credentials, go to your Twitter apps page. Here’s what the Apps page looks like:\n",
    "\n",
    "Edit app details Here you’ll find the Details button of your app. Clicking this button takes you to the next page, where you can generate the credentials.\n",
    "\n",
    "By selecting the Keys and tokens tab, you can generate and copy the key, token, and secrets to use them in your code:\n",
    "\n",
    "Generate keys and tokens After generating the credentials, save them to later use them in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read config from config.ini file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "api_key = config['twitter']['api_key']\n",
    "api_key_secret = config['twitter']['api_key_secret']\n",
    "access_token = config['twitter']['access_token']\n",
    "access_token_secret = config['twitter']['access_token_secret']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Authentication</h3>\n",
    "\n",
    "As we have previously seen, the Twitter API requires that all requests use OAuth to authenticate. So you need to create the required authentication credentials to be able to use the API. These credentials are four text strings:\n",
    "\n",
    "Consumer key Consumer secret Access token Access secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Tweepy to authenticate user using api key and access token\n",
    "auth = tweepy.OAuthHandler(api_key, api_key_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "\n",
    "# Checking if the api credentials is verified\n",
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"Authentication OK\")\n",
    "except:\n",
    "    print(\"Error during authentication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Extracting Tweets</h3>\n",
    "\n",
    "We are using search_tweets function to get all the tweets with keyword 'selling a table'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching tweets based on the text message\n",
    "tweets = api.search_tweets('selling a table',count=10000)\n",
    "\n",
    "# Checking the database connection \n",
    "try:\n",
    "    connection = mysql.connector.connect(host='localhost',\n",
    "                                         database='twitter_schema',\n",
    "                                         user='root',\n",
    "                                         password='root')\n",
    "    if connection.is_connected():\n",
    "        db_Info = connection.get_server_info()\n",
    "        print(\"Connected to MySQL Server version \", db_Info)\n",
    "        cursor = connection.cursor()\n",
    "        cursor.execute(\"select database();\")\n",
    "        record = cursor.fetchone()\n",
    "        print(\"You're connected to database: \", record)\n",
    "except Error as e:\n",
    "    print(\"Error while connecting to MySQL\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Loading the tweets into the MySQL database</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping over the entire tweets to fetch the required information and inserting the values in three twitter tables: tweet, tweet_mentions, tweet_tags\n",
    "for tweet in tweets:\n",
    "    tweet_id = tweet.id\n",
    "    created_at = tweet.created_at\n",
    "    twitter_text = tweet.text\n",
    "    username = tweet.user.screen_name\n",
    "    name = tweet.user.name\n",
    "    userId = tweet.user.id\n",
    "    follower_count = tweet.user.followers_count\n",
    "    following_count = tweet.user.friends_count\n",
    "    twitter_handle = tweet.user.screen_name\n",
    "    profile_image_url = tweet.user.profile_image_url_https\n",
    "    description = tweet.user.description\n",
    "    userCreated_at = tweet.user.created_at\n",
    "    status = api.get_status(tweet_id)\n",
    "    retweet_count = status.retweet_count \n",
    "    \n",
    "    cursor.execute('''insert into tweet ( twitter_handle, tweet_text, profile_image_url, tweet_date, user_created_at, retweets) values ( %s, %s, %s, %s, %s, %s)''', ( twitter_handle, twitter_text, profile_image_url, created_at,userCreated_at,retweet_count))\n",
    "    connection.commit()\n",
    "    if(len(tweet.entities['user_mentions']) > 0):\n",
    "        for mention in tweet.entities['user_mentions']:\n",
    "            target_user = mention['screen_name']\n",
    "            cursor.execute('''insert into tweet_mentions (tweet_id,source_user, target_user) values (%s, %s, %s)''', (cursor._last_insert_id,twitter_handle, target_user))\n",
    "    connection.commit()\n",
    "    if(len(tweet.entities['hashtags']) > 0):\n",
    "        for tag in tweet.entities['hashtags']:\n",
    "            tag = tag['text']\n",
    "            cursor.execute('''insert into tweet_tags (tweet_id,tag, target_user) values (%s,%s, %s)''', (cursor._last_insert_id,tag,target_user))\n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Fetch data and Audit Completeness and Consistency</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Source for flipkart csv dataset from kaggle: https://www.kaggle.com/datasets/PromptCloudHQ/flipkart-products\n",
    "# reading csv files\n",
    "flipkart_csv_df = pd.read_csv('C:\\\\Personal_Documents\\\\Assignments\\\\DMDD\\\\data_sources\\\\flipkart_com-ecommerce_sample.csv',delimiter = ',')\n",
    "\n",
    "# Source for orders csv from github: https://github.com/pawarbi/datasets/blob/master/Orders.csv\n",
    "# reading csv files\n",
    "order_df = pd.read_csv('C:\\\\Personal_Documents\\\\Assignments\\\\DMDD\\\\data_sources\\\\orders.csv',delimiter = ',')\n",
    "\n",
    "\n",
    "\n",
    "#Code to clean the data by removing all the na values, drop all the unwanted columns and removing all the duplicates\n",
    "flipkart_csv_df.drop(columns = [flipkart_csv_df.columns[0],flipkart_csv_df.columns[1],flipkart_csv_df.columns[2],flipkart_csv_df.columns[5],flipkart_csv_df.columns[6],flipkart_csv_df.columns[7],flipkart_csv_df.columns[8],flipkart_csv_df.columns[9],flipkart_csv_df.columns[10],flipkart_csv_df.columns[13],flipkart_csv_df.columns[14]],  inplace= True)\n",
    "flipkart_csv_df.dropna()\n",
    "flipkart_csv_df.drop_duplicates(subset=['product_name'])\n",
    "\n",
    "#Code to clean the data by removing all the na values, drop all the unwanted columns and removing all the duplicates. We have also used rename function to rename the columns.\n",
    "order_df.drop(columns = [order_df.columns[0],order_df.columns[1],order_df.columns[2],order_df.columns[4],order_df.columns[5],order_df.columns[6],order_df.columns[7],order_df.columns[8],order_df.columns[9],order_df.columns[10],order_df.columns[11],order_df.columns[12],order_df.columns[13],order_df.columns[14],order_df.columns[15],order_df.columns[17],order_df.columns[16],order_df.columns[19],order_df.columns[20],order_df.columns[21],order_df.columns[22],order_df.columns[23]],  inplace= True)\n",
    "order_df.rename(columns={\"Unit Price\": \"price\", \"Order Date\": \"order_date\"}, inplace=True)\n",
    "order_df.dropna()\n",
    "order_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Code block to insert records in the Category table</h3>\n",
    "We have written the code to insert the records in the category table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_id = 1\n",
    "for index,data in flipkart_csv_df.iterrows(): \n",
    "    if index == 500:\n",
    "        break \n",
    "    product_category_tree_df = data['product_category_tree']\n",
    "    #To fetch only the root value of the product_category_tree using the below logic\n",
    "    startString = '[\"'\n",
    "    endString = ' >>'\n",
    "    data['product_category_tree'] = product_category_tree_df[product_category_tree_df.find(startString)+len(startString):product_category_tree_df.find(endString)]\n",
    "    cursor.execute('''insert into category (category_id,category_name) values (%s,%s)''', (category_id,data[\"product_category_tree\"]))\n",
    "    category_id = category_id + 1\n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Code block to insert records in the Product table</h3>\n",
    "We have written the code to insert the records in the product table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_id = 1\n",
    "category_id = 1\n",
    "for index,data in flipkart_csv_df.iterrows():\n",
    "    if index == 500:\n",
    "        break \n",
    "    product_name_df = data['product_name']\n",
    "    data['product_name'] = product_name_df[:100]\n",
    "    cursor.execute('''insert into product (product_id,product_name,category_id) values (%s,%s,%s)''', (product_id, data[\"product_name\"],category_id))\n",
    "    connection.commit()\n",
    "    product_id = product_id + 1\n",
    "    category_id = category_id + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Code block to insert records in the Feedback table</h3>\n",
    "We have written the code to insert the records in the feedback table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_id = 1\n",
    "for index,data in flipkart_csv_df.iterrows():\n",
    "    if index == 500:\n",
    "        break\n",
    "# We will use proucting rating to generate value for the comments column. If the rating is not available we will assign -1 score. \n",
    "    product_rating_df = data['product_rating']\n",
    "    if product_rating_df == 'No rating available':\n",
    "        comments = 'No comments for the product'\n",
    "        feedback_score = -1\n",
    "    else:\n",
    "        feedback_score = round(float(product_rating_df.strip())) \n",
    "    \n",
    "\n",
    "    rounded_feedback_score = feedback_score\n",
    "\n",
    "# If the feedback score is less than 3, we will assign comment as 'Worst Product' else we will assing comment as 'Good Product'\n",
    "    if rounded_feedback_score < 3:\n",
    "        comments = 'Worst Product'\n",
    "    else:\n",
    "        comments = 'Good Product'\n",
    "        \n",
    "    cursor.execute('''insert into feedback (feedback_id,feedback_score,comments) values (%s,%s,%s)''', (feedback_id, feedback_score,comments))\n",
    "    connection.commit()\n",
    "    feedback_id = feedback_id + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Code to read universities and student datasets in dataframe. We have also cleaned and standardized the data by removing all the NULL and duplicate values.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Source for university dataset: https://public.opendatasoft.com/explore/dataset/us-colleges-and-universities/export/\n",
    "# reading csv files\n",
    "university = pd.read_csv('C:\\\\Personal_Documents\\\\Assignments\\\\DMDD\\\\data_sources\\\\us-colleges-and-universities.csv', sep=\",\")\n",
    "\n",
    "# formatting university data\n",
    "university = university[['OBJECTID', 'NAME', 'STATE', 'CITY']]\n",
    "university = university.rename(columns={\"OBJECTID\": \"university_id\", \"NAME\": \"university_name\", \"CITY\": \"city\", \"STATE\": \"state\"})\n",
    "\n",
    "#print(university.head(10))\n",
    "\n",
    "# Source for student dataset: https://github.com/ShapeLab/ZooidsCompositePhysicalizations/blob/master/Zooid_Vis/bin/data/student-dataset.csv\n",
    "# reading csv files\n",
    "student = pd.read_csv('C:\\\\Personal_Documents\\\\Assignments\\\\DMDD\\\\data_sources\\\\student.csv', sep=\",\")\n",
    "\n",
    "# formatting student data\n",
    "student = student[['id', 'name']]\n",
    "student[['first_name', 'last_name', 'temp']] = student.name.str.split(\" \", expand=True)\n",
    "student = student.drop('temp', axis=1)\n",
    "student = student.rename(columns={\"id\": \"student_id\"})\n",
    "student.dropna()\n",
    "\n",
    "# generating random university ids for student\n",
    "univ_id = []\n",
    "for i in range(len(student)):\n",
    "    univ_id.append(random.choice(university['university_id']))\n",
    "\n",
    "# adding university_id to student\n",
    "student['university_id'] = ''\n",
    "student['university_id'] = univ_id\n",
    "\n",
    "#Populating buyer\n",
    "buyer_id = random.sample(range(3, 100), 50)\n",
    "stud_id = []\n",
    "\n",
    "for i in range(50):\n",
    "    stud_id.append(random.choice(student['student_id']))\n",
    "\n",
    "temp_data = {'buyer_id': buyer_id,\n",
    "        'student_id': stud_id}\n",
    "\n",
    "buyer = pd.DataFrame(temp_data)\n",
    "\n",
    "#Populating seller\n",
    "seller_id = random.sample(range(103, 200), 50)\n",
    "premium_flag = []\n",
    "stud_id = []\n",
    "\n",
    "for i in range(50):\n",
    "    premium_flag.append(random.randint(0,1))\n",
    "\n",
    "for i in range(50):\n",
    "    stud_id.append(random.choice(student['student_id']))\n",
    "\n",
    "temp_data = {'seller_id': seller_id,\n",
    "            'student_id': stud_id,\n",
    "            'premium_flag':premium_flag}\n",
    "\n",
    "seller = pd.DataFrame(temp_data)\n",
    "seller.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Boxplot to view the consistency and range of data for the premium flag in the seller table</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot = seller.boxplot(column=['premium_flag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Code block to insert records in the University table</h3>\n",
    "We have written the code to insert the records in the university table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserting data into the database\n",
    "university_id = 1\n",
    "for uni in university.iterrows():\n",
    "    # university_id = uni[1][0]\n",
    "    university_name = uni[1][1]\n",
    "    state = uni[1][2]\n",
    "    city = uni[1][3]\n",
    "\n",
    "    cursor.execute(\n",
    "        '''insert into university (university_id, university_name, state, city) values ( %s, %s, %s, %s);''',\n",
    "        (index, university_name, state, city))\n",
    "    university_id = university_id + 1\n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Code block to insert records in the Student table</h3>\n",
    "We have written the code to insert the records in the student table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stud in student.iterrows():\n",
    "    student_id = stud[1][0]\n",
    "    university_id = stud[1][4]\n",
    "    first_name = stud[1][2]\n",
    "    last_name = stud[1][3]\n",
    "    cursor.execute(\n",
    "        '''insert into student (student_id, university_id, first_name, last_name) values ( %s, %s, %s, %s);''',\n",
    "        (student_id, university_id, first_name, last_name))\n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Code block to insert records in the Buyer table</h3>\n",
    "We have written the code to insert the records in the buyer table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in buyer.iterrows():\n",
    "    student_id = int(b[1][1])\n",
    "    buyer_id = int(b[1][0])\n",
    "    cursor.execute(\n",
    "        '''insert into buyer (buyer_id, student_id) values ( %s, %s);''',\n",
    "        (buyer_id, student_id))\n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Code block to insert records in the Seller table</h3>\n",
    "We have written the code to insert the records in the seller table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in seller.iterrows():\n",
    "    student_id = int(s[1][1])\n",
    "    seller_id = int(s[1][0])\n",
    "    premium_flag = int(s[1][2])\n",
    "\n",
    "    cursor.execute(\n",
    "        '''insert into seller (seller_id, student_id,premium_flag) values ( %s, %s, %s);''',\n",
    "        (seller_id, student_id, premium_flag))\n",
    "    connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Code block to insert records in the Tweet Order table</h3>\n",
    "We have written the code to insert the records in the tweet_order table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_order_id = 1\n",
    "tweet_id = 1\n",
    "product_id = 1\n",
    "feedback_id = 1\n",
    "\n",
    "seller_id = []\n",
    "buyer_id = []\n",
    "\n",
    "for i in range(len(order_df)):\n",
    "    seller_id.append(random.choice(seller['seller_id']))\n",
    "\n",
    "for i in range(len(order_df)):\n",
    "    buyer_id.append(random.choice(buyer['buyer_id']))\n",
    "\n",
    "temp_data = {\n",
    "        'seller_id' : seller_id,\n",
    "        'buyer_id' : buyer_id}\n",
    "\n",
    "main_df = pd.DataFrame(temp_data)\n",
    "\n",
    "result = pd.concat([main_df, order_df], axis=1)\n",
    "\n",
    "for index,data in result.iterrows(): \n",
    "    if index == 500:\n",
    "        break \n",
    "    print(result)\n",
    "    cursor.execute('''insert into twitter_order(t_order_id,buyer_id,seller_id,tweet_id,product_id,feedback_id,order_date, price) values (%s,%s,%s,%s,%s,%s,%s,%s)''', (t_order_id,data['buyer_id'],data['seller_id'],tweet_id,product_id,feedback_id,data['order_date'],data['price']))\n",
    "    t_order_id = t_order_id + 1\n",
    "    tweet_id = tweet_id + 1\n",
    "    product_id = product_id + 1\n",
    "    feedback_id = feedback_id + 1\n",
    "    connection.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fcc36d1ccd5caa3799214c4c78edfa7969ad6d25bcbeb3fef3413f17f0ea6e7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
